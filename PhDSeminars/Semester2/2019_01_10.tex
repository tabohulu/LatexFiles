\documentclass[fontsize=12pt]{article}
%\usepackage{xeCJK}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bm}
\usepackage[dvipdfmx]{graphicx}
%\setCJKmainfont{SimSun}
\title{Performance of Reed–Solomon codes using the
Guruswami–Sudan algorithm with improved
interpolation efficiency} 
\author{Kwame Ackah Bohulu}
\date{\today}
\begin{document}
\maketitle

\newpage
\section{Introduction}
The idea of list decoding block codes was introduced by
Elias [1] and Wozencraft [2] independently in the 1950s.
In 1997, Sudan [3] applied this idea to decode low-rate
$(n, k)$ RS codes beyond the half-distance boundary
$[(n-k-1)/2]$, where $n$ is the code length and $k$ is the
message length. Later, Guruswami and Sudan [5, 6]
improved the algorithm to decode RS codes of nearly any
code rate beyond this boundary. 
Unfortunately, this algorithm remained 
impractical to implement until Kotter and Vardy
[7–9] and Roth and Ruckenstein [10] presented lowcomplexity
implementation methods for the key steps of
the GS algorithm: interpolation and factorisation. In 2003,
McEliece [11] gave an explicit tutorial discussion of the
algorithm.

Traditional algebraic decoding algorithms for RS codes,including the Berlekamp–Massey algorithm [12] and
Euclid’s algorithm [13, 14]
generate a unique decoded codeword. They are very efficient in terms
of running time but are unable to correct any
number of errors greater than $[(n-k-1)/2]$ and therefore limits the
performance of RS codes over deeply corruptive channels.

The GS algorithm for RS codes removes this limitation by
finding a list of possible transmitted messages with decoding
considered to be successful as long as the transmitted
message is included in the list. The correct transmitted
message is chosen by re-encoding the list of candidate
messages and selecting the codeword with minimum
distance to the received word. The GS algorithm improves the error
correction capability significantly for low-rate $(1/3)$ RS
codes[5, 6]. However, for higher rate codes this algorithm can
still improve the error correction capability but with a less
significant improvement.

The GS algorithm has not been assessed by many
researchers due to its high decoding complexity and it
also requires a good understanding of mathematics.
However, its greater error-correction capability makes it is
a potential alternative decoding algorithm for RS codes
and can be extended to the family of algebraic–geometric
codes [5, 7], which lead to wider applications. 

This
paper describes the principle of the GS algorithm for
RS codes from an algebraic–geometric point of view.
Addressed towards improving the algorithm’s decoding
efficiency, a novel complexity-reduced modification to
the original algorithm is presented and a detailed
complexity analysis is given


\section{Overview of Guruswami - Sudan Algorithm}
The table below shows the commonly used notations in this paper and their meanings 

\begin{center}
\begin{tabular}{ | c | c | }

\hline
 Symbol & Meaning \\ 
 \hline \hline
 $\mathbb{F}_q$ & finite field with $q$ elements \\  
 \hline
 $\mathbb{F}_q[x]$ & ring of polynomials with coefficients from $\mathbb{F}_q$ and
variable $x$ \\ 
 \hline
 $\mathbb{F}_q[x^w]$ & ring of polynomials from $\mathbb{F}_q$ with $x$  degree $\leq w$\\  
 \hline
 $\mathbb{F}_q[x,y]$ & ring of nivariate polynomials with coefficients from $\mathbb{F}_q$ and
variables $x $ and $y$\\
 \hline    
\end{tabular}
\end{center}

Assuming a function $f(x)$ which is a subspace of $\mathbb{F}_q[x^{k-1}]$, a $(n,k)$ RS code is generated by evaluating $f(x)$ at a set of points $x_0,x_1,...,x_{n-1} \in \mathbb{F}_q$

$$ (c_0,c_1,...,c_{n-1}) =(f(x_0),f(x_1),...,f(x_{n-1}))$$

it should be noted that 

$$ f(x) = f_0+f_1x+\cdot\cdot\cdot+f_{k-1}x^{k-1}$$
where the coefficients $f_0,f_1,...,f_{k-1} \in \mathbb{F}_q$ are considered as the transmitted message.


\subsection{Brief Description of the Guruswami-Sudan Algorithm}
The above mentioned algorithm is made up of two major steps, namely Interpolation and Factorization.

\begin{itemize}
\item Interpolation: From the encoding process, it can be shown that evey member of the received word $y_i$ was formed by a corresponding finite field element $x_i,\,\,\, 0\leq i \leq n-1$. it is therefore possible to form $n (x_i,y_i)$ point pairs. In the interpolation step,we seek to construct a bivariate polynomial 
\begin{equation}
Q(x,y)= \sum_{i,j}^{}q_{ij}x^iy^i
\end{equation}
which has a minimal $(1,k-1)$- weight degree and has a zero order $m$ over these $n$ points. $m$ which is called the multiplicity is the number of times the polynomial intersects the $n$ points.
\item Factorization: After the polynomial $Q(x,y)$ is found factorization is done to find the list $L$ of possible transmitted message polynomials $p(x)$ and is given by
\begin{equation}L= \{p(x):(y-p(x)) | Q(x,y) \text{ and }  \text{deg}(p(x)) < k \}\end{equation} The one with the minimum distance from the received message after re-encoding is chosen as the transmitted message.
\end{itemize}

\subsection{Decoding Parameters}
Necessary decoding parameters for understanding the GS algorithm are introduced here.
\begin{itemize}
\item We define the $(u,v)$-weight  degree of monomial $x^iy^i$ as 
\begin{equation}
w-\text{deg}_{u,v}(x^iy^j) = iu+jv
\end{equation}
This is used to sort monomials in a desired order. In this paper, monomials are arranged according to the $(1,k-1)$- reverse lexicographical ($(1,k-1)$- revlex) order. The sorting rule used is shown below


$$x^{i_1}y^{j_1} < x^{i_2}y^{j_2} \text{ if } $$
$$ w-\text{deg}_{1,k-1}(x^{i_1}y^{j_1}) < w-\text{deg}_{1,k-1}(x^{i_2}y^{j_2}) $$
$$\text{ or } w-\text{deg}_{1,k-1}(x^{i_1}y^{j_1}) = w-\text{deg}_{1,k-1}(x^{i_2}y^{j_2}) \text{ and } i_1 >i_2$$

Using the decoding of the $(7,5)$ RS code as an example, the $(1,4) $ weight degree and $(1,4)$ revlex order monomials $x^iy^j$ are shown in Tables 1 and 2. From table 2, we can see that $\text{ord}(x^4)=4,\text{ord}(x^2y)=9 \text{ and } \text{ord}(y^2)=4$ and therefore $x^4< x^2y < y^2$. ord$(x^iy^i) $ is the  $(1,k-1)$ revlex order of the monomial $x^iy^i$

\item We define the weight degree of a nonzero bivariate polynomial $Q(x,y)$ as the weight degree of its leading monomial $M_L$
ie 
\begin{equation}
Q(x,y) = a_0M_0+\cdot\cdot\cdot+a_LM_L\text{, with 		} \\
M_0<...<M_L \text{ and } a_0,...,a_L \in \mathbb{F}_q, a_L\neq 0
\end{equation} 
and
\begin{equation}
 w-\text{deg}_{1,k-1} (Q(x,y)) =  w-\text{deg}_{1,k-1}(M_L)
 \label{5}
\end{equation}
\item $L= \text{lod}(Q(X,Y)) =\text{ord}(M_L)$ is called the leading order( lod) of $Q(X,Y)$. it is possible to compare two polynomials by comparing their lod
\item $S_x(N) \text{ and } S_y(N) $ are denoted as the highest degree of $x$ and $y$ under the $(1,k-1) $- revlex order s.t. 
\begin{equation}
S_x(N) = \max{\{ i: \text{ord}(x^iy^0) \leq N  \}}
\end{equation}
\begin{equation}
S_y(N) = \max{\{ i: \text{ord}(x^0y^j) \leq N  \}}
\end{equation}
Where $N$ is any non-negative integer 
\item it is possible to rewrite (\ref{5}) as 
\begin{equation}
 w-\text{deg}_{1,k-1} (Q(x,y)) = S_x(L) 
\end{equation}
since under the $(1, k - 1)$-revlex order, $x^iy^0$ is the minimal monomial
with weight degree $i$.
\item The error correction capability $t_m$ and the maximum number of candidate messages $l_m$  with respect to a given multiplicity $m$ are defined as
\begin{equation}
t_m = n-1 -\Big\lfloor\frac{S_x(C)}{m}\Big\rfloor
\end{equation}

\begin{equation}
l_m = S_y(C)
\label{10}
\end{equation}

\begin{equation}
C =n\binom{m+1}{2}
\end{equation}
The above parameters grow monotonically with multiplicity $m$
\item $t_{m_{GS}} = n-1 -\Big\lfloor\sqrt{(k-1)n}\Big\rfloor$ and is defined as the upper bound on the error-correcting capability of the GS algorithm. This is implies that there is also an upper bound on the value of $m \text{, }(m_{GS})$
$t_{m_{GS}}$ is greater than or equal to $\Big\lfloor(n-k-1)/2\Big\rfloor$
\end{itemize}

Two examples are given to show how $t_m$ and $l_m$ grow with multiplicity $m$


\section{Interpolation}
In this section, the interpolation theorem is explained from the algebraic geometric point of view, followed by a detailed description of Kotter's interpolation algorithm and a modification to improve its efficiency.

\subsection{Interpolation Theorem}
In the case of RS codes $1,x,...,x^a$ are the rational functions that have increasing pole order over the point of infinity $p_{\infty}$ of a projective curve. In general, interpolated polynomials can be written as 

\begin{equation}
Q(x,y) = \sum_{a,b \in \mathbb{N}}^{} q_{ab}x^ay^b \text{, } q_{ab} \in \mathbb{F}_q
\label{12}
\end{equation}

Again, functions $1, (1-x_i),...,(1-x_i)^u$ are the rational functions that have
increasing zero order over the finite-field element $x_i$ used in
encoding, and the received word $y_i \in \mathbb{F}_q$ It is possible to write the interpolated polynomial with respect to $(x_i,y_i) $ as 

\begin{equation}
Q(x,y) = \sum_{u,v \in \mathbb{N}}^{} q_{uv}^{(x_i,y_i)}(x-x_i)^u (y-y_i)^v \text{, } q_{uv}^{(x_i,y_i)} \in \mathbb{F}_q
\label{13}
\end{equation}

if $q_{uv}^{(x_i,y_i)}= 0 \text{ for } u + v < m, Q(x,y) $ has a multiplicity of $m$ over $(x_i,y_i)$
Notice that 
\begin{equation}
x^a = (x -x_i+x_i) = \sum_{a \geq u} \binom{a}{u} x_i^{(a-u)}(x - x_i)^u
\label{14}
\end{equation}
and 
\begin{equation}
y^a = (y -y_i+y_i) = \sum_{b \geq v} \binom{b}{v} y_i^{(b-v)}(y - y_i)^v
\label{15}
\end{equation}

substitutiing (\ref{14}) and (\ref{15}) into (\ref{12})  gives
\begin{equation}
Q(x,y) = \sum_{u,v} \sum_{a \geq u,b \geq v} q_{ab}\binom{a}{u} \binom{b}{v} x_i^{(a-u)}y_i^{(b-v)}\\
(x - x_i)^u  (y - y_i)^v
\end{equation}
which means 
\begin{equation}
q_{uv}^{(x_i,y_i)} =\sum_{a \geq u,b \geq v} q_{ab}\binom{a}{u} \binom{b}{v} x_i^{(a-u)}y_i^{(b-v)}
\label{17}
\end{equation}
(\ref{17}) is  (u, v)-Hasse derivative evaluation on the point
$(x_i, y_i)$ of the polynomial $Q(x, y)$ defined by (1\ref{12}) [17, 20,
21]. Using $D(Q)$ to denote the Hasse derivative evaluation
of $Q(x, y)$, (\ref{17}) can be denoted as

\begin{equation}
D_{uv}Q{(x_i,y_i)} =\sum_{a \geq u,b \geq v} q_{ab}\binom{a}{u} \binom{b}{v} x_i^{(a-u)}y_i^{(b-v)}
\label{18}
\end{equation}
From the above analysis, the interpolation of the GS algorithm can be
generalised as: Find a minimal $(1, k-1)$ - weight degree
polynomial $Q(x, y)$ that satisfies

\begin{equation}
Q(x,y) = \min\{Q(x,y) \in \mathbb{F}_q[x,y]  | D_{u,v}Q(x_i,y_i) = 0 \text{ for } i = 0,...,n-1 \text{ and } u+v < m, (u,v \in \mathbb{N}) \}
\label{19}
\end{equation}

\subsection{Kotter's Algorithm}
The algorithm suggested by Kotter[6-8] provides an efficient method forpolynomial reconstruction. It is an iterative modification algorithm based of these 2 properties of the Hasse Derivative
\paragraph{Property 1: } Linear function of the Hasse derivative\\ If $H,Q \in \mathbb{F}_q[x,y],  \,\, c_1\text{ and } c_2 \in \mathbb{F}_q$, then
\begin{equation}
D(c_1H + c_2Q) = c_1D(H) + c_2D(Q)
\end{equation}

\paragraph{Property 2: Bilinear Hasse Derivative} If $H,Q \in \mathbb{F}_q[x,y]$,  then
\begin{equation}[H,Q]_D = HD(Q) - QD(H)\label{21}\end{equation}

if the Hasse derivative evaluation of $D(Q) =d_1 \text{ and } D(H) =d_2 (d_1,d_2 \neq 0)$, based on Property 1 it is obvious to conclude that 
\begin{equation}[H,Q]_D =0 \label{22}\end{equation}
If lod$(H) >$ lod$(Q)$, the new constructed polynomial from
(28) has leading order lod$(H)$. Therefore by performing
the bilinear Hasse derivative over two polynomials both
of which have nonzero evaluations, one can reconstruct a
polynomial which has zero Hasse derivative evaluation.
Based on this principle, Kotter’s algorithm is to iteratively
modify a set of polynomials through all n points and with
every possible (u, v) pair under each point.

With multiplicity $m$, there are $\binom{m+1}{2}$ pairs of $(u,v)$. They are arranged as follows

$$ (u,v) =(0,0),(0,1),...,(0,m-1), (1,0),...(1,m-2),...,(m-1,0) $$

This means that there will be $C=n\binom{m+1}{2}$ iterative modifications required for a given RS code. The index for the iterative modifications is given by $i_k, i_k=0,1,...C$

\subsubsection{Procedure for Kotter Algorithm}
\begin{itemize}
\item The initial step is initialize a group of polynomials $G_0$ (ie $i_k=0$) as
\begin{equation}
G_0=\{g_{0,j} =y^j, \,\,\,\, j=0,1,...l_m\}
\end{equation}
where $l_m$ is defined in (\ref{10}).
It is important to point out that 
\begin{equation}
g_{0,j} =\min\{g(x,y) \in \mathbb{F}_q[x,y]|\text{deg}_y(M_L) =j \}
\end{equation}
Where $M_L$ is the leading order of $g$
\item Next each is tested using (\ref{18}) by
\begin{equation}
\Delta_j =D_{i_k=0}(g_{i_k,j})
\end{equation}
if $\Delta_j =0$ no further modifications are required
\item if $\Delta_j \neq 0$ the polynomial is modified using the Bilinear Hasse derivative defined in (\ref{21})
\item To construct a group of polynomials which satisfy 
\begin{equation}
\begin{split}
g_{i_k+1,j} =&\min \Bigg\{   g(x,y) \in \mathbb{F}_q[x,y] | D_{i_k,j}(g_{i_k+1,j})=0,\\&D_{i_k-1,j}(g_{i_k+1,j})=0,...,D_{0,j}(g_{i_k+1,j})=0\\
&\text{ and } deg_y(M_L)=j
 \Bigg\}
 \end{split}
\end{equation}
we choose the minimal polynomial among those polynomials
with $\Delta_j neq 0$, denote its index as $j^*$ and record it
as $g^*$
\begin{equation}
\begin{split}
&j^* =\text{index} (\min{g_{i_k,j} | \Delta_j \neq 0})\\
&g^* =g_{i_k,j^*}
\end{split}
\end{equation}
\item For those polynomials with $\Delta_j \neq 0 \text{ but } j \neq j^*$ modify them using the Bilinear Hasse derivative defined in (\ref{21}) without increasing the leading order.
\begin{equation}
g_{i_k+1,j} = [g_{i_k,j}, g^*]_{D_{i_k}}
\end{equation}
\item for $g^*$ we modify it (\ref{21}) while increasing the leading order.
\begin{equation}
g_{i_k+1,j^*} = [xg^*, g^*]_{D_{i_k}}
\end{equation}
\item After $C$ iterative modifications the
minimal polynomial in $G_C$ is the interpolated polynomial that
satisfies (\ref{19}), and it is chosen to be factorised in the next step
\end{itemize}






\end{document}