\documentclass[fontsize=12pt]{article}
%\usepackage{xeCJK}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage[dvipdfmx]{graphicx}
%\setCJKmainfont{SimSun}
\title{Nonsystematic Convolutional Codes for Sequential Decoding In Space Applications} 
\author{Kwame Ackah Bohulu}
\date{\today}
\begin{document}
\maketitle

\newpage


\section{Introduction}

\begin{enumerate}

\item {About the Deep-Space Channel}
\begin{itemize}
\item Channel Model: Additive White Gaussian Noise Channel

\item Decoder for Convolutional Codesl: Sequential Decoder(Best at that time)
\end{itemize}


\item {Purpose of Paper}
\begin{itemize}
\item Introduce New class of Convolutional codes

\item Has Lower BER and lower implementation complexity than what is used currently
\end{itemize}

\item {Terms to be used in the Paper}
\begin{itemize}
\item $\mathbf{x}=\{x_0,x_1,x_2,...\}$ : Binary information bit sequence. In polynomial form (D transform) it may be written as $\mathbf{X}(D)=x_0+x_1D+x_2D^2+.....$

\item rate $(\mathbf{R}=1/n)$ convolutional encoder with memory order $m$: convolutional encoder with single input and $n$ outputs. 

\item The $n$ outputs are determined by the $n$ transfer functions (also known as the generator functions) . The "i"th transfer function is written as $\mathbf{\mathbf{\mathbf{G}}}^{(i)}(D)=g_0^{i}+g_1^{i}D+...+g_m^{i}D^m$

\item The "i"th input bit is is used to produce the "j"th encoded sequence $\mathbf{C}^{(j)}(D)=c^{i}_0+c^{i}_1D+c^{i}_2D^2+...$ 

\item in compact form 
\begin{equation}
\mathbf{C}^{(j)}(D)=\mathbf{X}(D)\mathbf{\mathbf{G}}^{(i)}(D)
\end{equation}
All additions and multiplications are done in modulo 2.

\item A convolutional code is said to be systematic if  
\begin{equation}
\mathbf{C}^{(1)}(D)=\mathbf{X}(D)~\text{or} ~\mathbf{\mathbf{G}}^{(1)}(D)=1
\end{equation}

\item initial code word[2] : span of $(m+1)$n encoded digits generated by  the encoder with respect to the first information bit $x_0$

\item minimum distance ($d_m$): Fewest number of positions in which two initial code words arising from different values of $i_0$ are different. This is equivalent to the fewest number of nonzero digits in any initial code word with $i_0 = 1$ 
\end{itemize}

\end{enumerate}


\section{Systematic Versus Non-Systematic Codes}
Let the "j"th hard decisioned signal at the receiver be expressed as 
$\mathbf{\mathbf{R}}^{(j)}(D)=r^{i}_0+r^{i}_1D+r^{i}_2D^2+...$ which may be rewritten as 
\begin{equation}
\mathbf{\mathbf{R}}^{(j)}(D)=\mathbf{C}^{(j)}(D)+\mathbf{\mathbf{E}}^{(j)}(D)
\end{equation} 
where $\mathbf{\mathbf{E}}^{(j)}(D)=e^{i}_0+e^{i}_1D+e^{i}_2D^2+...$ is the sequence of errors in the hard decisions.
\begin{enumerate}
\item Why systematic codes over non systematic codes?

\begin{itemize}
\item Easy to get estimates of information sequence for synchronization purposes without decoding, ie $\mathbf{\mathbf{R}}^{(1)}(D)=\mathbf{X}(D)+\mathbf{\mathbf{E}}^{(1)}(D)$

\item According to [3], for any non-systematic codeword, there exists a systematic code word with the same minimum distance $d_m$. Therefore, there is no extra advantage when using threshold decoders. Threshold decoders make decisions on $i_0$ based on the initial codeword only[2].

\item There is a fear that non-systematic encoders are more difficult to implement and that catastrophic errors may occur from the recovery of the data.
\end{itemize}

\item Why non-Systematic codes?

\begin{itemize}
\item According to results from[4]-[6] they have a naturally Lower error probability than that of systematic codes when used with sequential decoding [7],[8] and maximum likelihood decoders[9].

\item The error probalility for such decoders is closely related to the free distance of the code which is equivalent to the fewest number of non-zero elements in a encoded sequence.

\item According to [5], for a given memory order $m$, non-systematic codes provide more free distance than their systematic equivalents
\end{itemize}

\end{enumerate}

Based on these considerations, the author seeks to design non-systematic convolutional codes which has good performance with sequential decoding and has the ability to extract the information sequence with as much ease as possible. 

\section{Quick-Look-In Non-systematic Codes}
\begin{enumerate}
\item According to [10], a convolutional encoder with a feedfoward (FF) inverse is a necessary and sufficient condition to avoid catastrophic error propagation.

\item FF inverse for $\mathbf{R}=1/n$ encoder : $n$-input single output linear sequential circuit with polynomial transfer function $\mathbf{\mathbf{P}}_j(D), j=1,2,...,n$ such that 

\begin{equation}
\sum_{j=1}^{n} \mathbf{\mathbf{P}}_i(D)\mathbf{C}^{(j)}(D) = D^L\mathbf{X}(D)
\end{equation}

\item since $\mathbf{C}^{(j)}(D)=\mathbf{X}(D)\mathbf{\mathbf{G}}^{(i)}(D)$, the above equation may be rewritten as 
\begin{equation}
\sum_{i=1}^{n} \mathbf{\mathbf{P}}_i(D)\mathbf{\mathbf{G}}^{(i)}(D) = D^L
\end{equation}

\item Case where $R$
\begin{itemize}
\item We may rewrite eqn $(4)$ and $(5)$ as

\begin{equation}
\mathbf{\mathbf{P}}_1(D)\mathbf{C}^{(1)}(D) +\mathbf{\mathbf{P}}_2(D)\mathbf{C}^{(2)}(D)= D^L\mathbf{X}(D)
\end{equation}

and

\begin{equation}
\mathbf{\mathbf{P}}_1(D)\mathbf{G}^{(1)}(D) + \mathbf{\mathbf{P}}_2(D)\mathbf{G}^{(2)}(D)= D^L
\end{equation}

\item estimate of $\mathbf{X}(D)$ can be obtained by passing the hard decisioned received signal through the FF inverse, ie

\begin{equation}
\mathbf{P}_1(D)\mathbf{R}^{(1)}(D) +\mathbf{P}_2(D)\mathbf{R}^{(2)}(D)= D^L[\mathbf{X}(D)+\mathbf{\Delta}(D)]
\end{equation}
where $$\mathbf{\Delta}(D)=\delta_0 +\delta_1D + \delta_2D^2+...$$ is the sequence of errors in the estimated information digits.

\item using the fact that $\mathbf{\mathbf{R}}^{(j)}(D)=\mathbf{C}^{(j)}(D)+\mathbf{\mathbf{E}}^{(j)}(D)$, we obtain the equation below.

\begin{equation}
\mathbf{P}_1(D)\mathbf{E}^{(1)}(D) +\mathbf{P}_2(D)\mathbf{E}^{(2)}(D)= D^L\mathbf{\Delta}(D)
\end{equation}

This equation relates the errors in the hard decisioned received signal to the errors in the estimated information digits.

\item If we assume that the individual hard decisioned received bits have a probability $p$, it follows from (9) that  if $p<<1$ then $\delta_i=1$ with probability 

\begin{equation}
p_{\delta} = \{W[\mathbf{P}_1(D)]+W[\mathbf{P}_2(D)]\}p
\end{equation}
where $W[\mathbf{P}_j(D)]$ is the hamming weight of the $[\mathbf{P}_j(D)]$. we refer to 

$$ A=\{W[\mathbf{P}_1(D)]+W[\mathbf{P}_2(D)]\}$$ as the error amplification factor
\end{itemize}

\item For systematic codes $A$ takes on the the minimum possible value of $1$, where $\mathbf{P}_1(D)=1$ and where $\mathbf{P}_2(D)=0$

\item For non systematic codes the minimum possible value for $A$ is $2$, where $\mathbf{P}_1(D)=\mathbf{P}_2(D)=1$

\item For such a code, we may rewrite (8) as 
\begin{equation}
\mathbf{R}^{(1)}(D) + \mathbf{R}^{(2)}(D)= D^L[\mathbf{X}(D)+\mathbf{\Delta}(D)]
\end{equation}
which means that the estimate of the information sequence is obtained by the modulo 2 addition of the received sequences. It is worth noting that  $\mathbf{P}_1(D)=\mathbf{P}_2(D)=1$ is only possible if 

\begin{equation}
\mathbf{G}^{(1)}(D)+\mathbf{G}^{(2)}(D)=D^L
\end{equation}

and any rate $\mathbf{R}=1/n$ non-systematic code which satisfies this condition is known as a quick-look-in code.


\end{enumerate}
\section{Sequential Decoding and Consideration And Code Construction}
When it comes to sequential decoding of convilutional codes, there are two important factors to be considered. The distribution of computation and the undetected error probability.
\begin{enumerate}
\item Distribution of computation : This value happens to be a random number and the code parameter which is closely related to this is know as the column distance $d_k,~k=0,1,...$

\begin{itemize}
\item By definition[11], the column distance is minimum distance of  the code of memory order $k$ obtained by dropping all terms of degree greater than $k$ from the original code generating polynomial.

\item Examples : For a rate $R$ convolutional encoder, assume we have $\mathbf{G}^{(1)}(D)=g^{(1)}_0 + g^{(1)}_1D+g^{(1)}_2D^2$ and $\mathbf{G}^{(2)}(D)=g^{(2)}_0 + g^{(2)}_1D+g^{(2)}_2D^2$. 

\item We see that $d_0=2$~ iff $g^{(1)}_0=g^{(2)}_0$ and $d_1=3$ iff $d_0 =2$ and $g^{(1)}_1 \neq g^{(2)}_1$

According to [8] simulations have shown that for $d_1<3$ the amount of computation required is unaceptablly large and thus in the construction of the quick-look-in codes, the search is restricted to $d_1>3$. Specifically

$$g^{(1)}_0 + g^{(1)}_1 = 1$$ and $$g^{(2)}_0 + g^{(2)}_1 = 1 + D$$

\item furthermore, simulations have shown that for good computational performance $d_2,d_3,...$ should grow as rapidly as possible to minimize the need for long searches in the code trellis. This means the values for $g^{(1)}_2,g^{(1)}_3,...g^{(1)}_m$ and  $g^{(2)}_2,g^{(2)}_3,...g^{(2)}_m$ should be chosen to maximize the values of the higher column distances.

\item Unfortunately, this does not uniquely define the code since many different combinations can lead to the same value of $d_k$
\end{itemize}
\item Undetected error probability

\begin{itemize}
\item This parameter is most affected by the free distance of the code which is in turn related to a large number of "1"'s in the generator polynomials.

\item This suggest that for the same value of $d_k$, you should choose the one which has the larger number of "1"'s in the generator polynomial.

\end{itemize}
\item with the above considerations in made, a unique QLI code can be constructed using the algorithm below

\begin{enumerate}
\item choose $g_0^{(1)} = g_0^{(2)}  =1$ and  $g_1^{(1)} =0$. Set $d_1=3,k=2$ 

\item set $g_k^{(1)} = g_k^{(2)}=0$ and compute $d_k$. if $d_k > d_{k-1}$ go to step (d).

\item set $g_k^{(1)} = g_k^{(2)}=1$ 

\item if $k=m$, stop. Otherwise $k=k+1$ and go to step (b)
\end{enumerate}

\item Using the above algorithm, the code with memory order $m=35$ has the following generator polynomials written in octal notation 

$$ \mathbf{G}^{(1)}=[533,533,678,737]_8$$ and $$ \mathbf{G}^{(2)}=[733,533,678,737]_8$$
\end{enumerate}

\section{Performance with Sequential Decoding}
The QLI code for $m=35$ obtained using the previously stated algorithm is compared with the best known $m=35$ systematic code which has genetator polynomials $$ \mathbf{G}^{(1)}=[400,000,000,000]_8$$ and $$ \mathbf{G}^{(2)}=[715,473,701,317]_8$$

\begin{enumerate}
\item Simulation parameters: 

\begin{itemize}
\item Channel models: AWGN and Binary Symetric channel

\item Frame Length :$256$, with $35$ extra zero bits to force encoder back to zero state.

\item Decoding algorithm: Fano sequential algorithm with maximum of $50,000$ computations per frame. $1,000$ frames decoded for each channel model

\end{itemize}

\item In conclusion, the QLI codess give comparable computational performance to that of the systematic code with the same memory order, but have a lower undetected error probability.
\end{enumerate}

\section{Encoder Implementation}
\begin{enumerate}
\item To implement any convolutional encoder requires $W[\mathbf{G}^{(1)}(D)]+W[\mathbf{G}^{(2)}(D)] -2 $ two-input modulo 2 adders. This is shown in fig. 1

\item This means the QLI requres $53$ modulo 2 adders whiles the systematic code requires $21$.

\item a simple trick can be used to deal with the large number of modulo 2 adders required. All we need to do is to implement the binary compliment of the desired encoder and and add a circuit that does the necessary complementation at the output.

\item Let $\bar{\mathbf{G}}^{(1)}(D)=\bar{g}_0 + \bar{g}_1D+\bar{g}_2D^2+...+\bar{g}_mD^m$, where $\bar{g}_i  = g_1 +1$, we may write 

$$ \mathbf{G}(D) =\bar{\mathbf{G}}^{(1)}(D)+ 1+D+...+D^m $$
or 
$$ \mathbf{G}(D) =\bar{\mathbf{G}}^{(1)}(D)+( 1+D^{m+1})/(1+D) $$

\item This means that the required number of modulo 2 adders is now given by $W[\bar{\mathbf{G}}(D)]+2 =m+2-W[\mathbf{G}(D)] $ and the circuit looks that in fig. 2

For the QLI code used in the simulation, the number of modulo 2 adders to implement $\mathbf{G}^{(2)}(D)$ and $\mathbf{G}^{(1)}(D)$ reduces to $10$ and $11$ respectively.  

\item In conclusion, the circuit required for the implementation of the QLI isn't too complicated when compared to that of the systematic code with the same memory order.
\end{enumerate}

\section{Conclusions}

\begin{enumerate}
\item The reason for the design of QLI was explained and an algorithm for the construction of such codes was introduced for $R=1/2$.

\item it was shown that these codes have a lower undetected error probability for sequential decoding when compared to systematic codes of the same order.

\item It was shown that the encoder construction was simpler that that of the systematic encoder construction


\end{enumerate}
\end{document}